{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import glob\n",
    "import csv\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()  # use directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataframe Train Negatif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field = [\"review\"];\n",
    "# test = pd.read_csv('aclImdb/train/neg/10402_1.txt',header = None,quoting=csv.QUOTE_NONE, \n",
    "#                  encoding ='utf-8',names=field, index_col=None)\n",
    "test = pd.read_csv('aclImdb/train/neg/10402_1.txt',delimiter = \"\\t\")\n",
    "test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fields = [\"review\"];\n",
    "l = [pd.read_csv(filename,header = None,quoting=csv.QUOTE_NONE, \n",
    "                 encoding ='latin1',names=fields, index_col=None,delimiter = \"\\t\") for filename in glob.glob(\"aclImdb/train/neg/*.txt\")]\n",
    "\n",
    "dfNegatif = pd.concat(l, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Working with one of the best Shakespeare sourc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well...tremors I, the original started off in ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ouch! This one was a bit painful to sit throug...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I've seen some crappy movies in my life, but t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Carriers\" follows the exploits of two guys an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  Working with one of the best Shakespeare sourc...          0\n",
       "0  Well...tremors I, the original started off in ...          0\n",
       "0  Ouch! This one was a bit painful to sit throug...          0\n",
       "0  I've seen some crappy movies in my life, but t...          0\n",
       "0  \"Carriers\" follows the exploits of two guys an...          0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfNegatif['sentiment'] = 0\n",
    "dfNegatif.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataframe Train Positif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review       For a movie that gets no respect there sure ar...\n",
       "sentiment                                                    1\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column = [\"review\"];\n",
    "l = [pd.read_csv(filename,header = None,quoting=csv.QUOTE_NONE, encoding='utf-8',names=column, index_col=None,delimiter = \"\\t\") for filename in glob.glob(\"aclImdb/train/pos/*.txt\")]\n",
    "dfPositif = pd.concat(l, axis=0)\n",
    "dfPositif['sentiment'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>For a movie that gets no respect there sure ar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bizarre horror movie filled with famous faces ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A solid, if unremarkable film. Matthau, as Ein...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It's a strange feeling to sit alone in a theat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You probably all already know this by now, but...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  For a movie that gets no respect there sure ar...          1\n",
       "0  Bizarre horror movie filled with famous faces ...          1\n",
       "0  A solid, if unremarkable film. Matthau, as Ein...          1\n",
       "0  It's a strange feeling to sit alone in a theat...          1\n",
       "0  You probably all already know this by now, but...          1"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfPositif.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Combine and shuffle data trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16397</th>\n",
       "      <td>My all-time favorite movie! I have seen many m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4850</th>\n",
       "      <td>This movie is so bad, it can only be compared ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23082</th>\n",
       "      <td>In the history of movies based on comic books,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6553</th>\n",
       "      <td>Once upon a time there was a director by the n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13088</th>\n",
       "      <td>Holly addresses the issue of child sexploitati...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14421</th>\n",
       "      <td>Sort of like a very primitive episode of \"Gene...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10706</th>\n",
       "      <td>This movie proves that you can't judge a movie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12629</th>\n",
       "      <td>Cute film about three lively sisters from Swit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8090</th>\n",
       "      <td>When someone remakes a classic movie, the rema...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5375</th>\n",
       "      <td>Although Robert \"Knox\" Benfer has his fans, I'...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16499</th>\n",
       "      <td>This is one of my favorite Govinda movies of a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>I gave this movie a very fair chance, and it b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22621</th>\n",
       "      <td>I saw this movie years ago on late night telev...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7592</th>\n",
       "      <td>My life is about saving animals. I do voluntee...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2852</th>\n",
       "      <td>My one line summary should explain it all, but...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "16397  My all-time favorite movie! I have seen many m...          1\n",
       "4850   This movie is so bad, it can only be compared ...          0\n",
       "23082  In the history of movies based on comic books,...          1\n",
       "6553   Once upon a time there was a director by the n...          0\n",
       "13088  Holly addresses the issue of child sexploitati...          1\n",
       "14421  Sort of like a very primitive episode of \"Gene...          1\n",
       "10706  This movie proves that you can't judge a movie...          0\n",
       "12629  Cute film about three lively sisters from Swit...          1\n",
       "8090   When someone remakes a classic movie, the rema...          0\n",
       "5375   Although Robert \"Knox\" Benfer has his fans, I'...          0\n",
       "16499  This is one of my favorite Govinda movies of a...          1\n",
       "106    I gave this movie a very fair chance, and it b...          0\n",
       "22621  I saw this movie years ago on late night telev...          1\n",
       "7592   My life is about saving animals. I do voluntee...          0\n",
       "2852   My one line summary should explain it all, but...          0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataTrain = dfNegatif.append(dfPositif, ignore_index=False, verify_integrity=False, sort=True)\n",
    "dataTrain = shuffle(dataTrain)\n",
    "dataTrain.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['could',\n",
       " 'even',\n",
       " 'enough',\n",
       " 'few',\n",
       " 'still',\n",
       " 'him',\n",
       " 'get',\n",
       " 'or',\n",
       " 'were',\n",
       " 'on',\n",
       " 'why',\n",
       " 'before',\n",
       " 'put',\n",
       " 'somehow',\n",
       " 'one',\n",
       " 'its',\n",
       " 'formerly',\n",
       " 'just',\n",
       " 'somewhere',\n",
       " 'hers',\n",
       " 'your',\n",
       " 'top',\n",
       " 'has',\n",
       " 'whither',\n",
       " 'cannot',\n",
       " 'yourselves',\n",
       " 'amount',\n",
       " 'show',\n",
       " 'thereby',\n",
       " 'should',\n",
       " 'am',\n",
       " 'hereupon',\n",
       " 'whence',\n",
       " 'ourselves',\n",
       " 'whole',\n",
       " 'latter',\n",
       " 'becoming',\n",
       " 'whereas',\n",
       " 'into',\n",
       " 'unless',\n",
       " 'i',\n",
       " 'everywhere',\n",
       " 'than',\n",
       " 'together',\n",
       " 'latterly',\n",
       " 'but',\n",
       " 'more',\n",
       " 'after',\n",
       " 'someone',\n",
       " 'take',\n",
       " 'hence',\n",
       " 'last',\n",
       " 'often',\n",
       " 'other',\n",
       " 'what',\n",
       " 'whoever',\n",
       " 'perhaps',\n",
       " 'as',\n",
       " 'by',\n",
       " 'now',\n",
       " 'seeming',\n",
       " 'thereupon',\n",
       " 'toward',\n",
       " 'would',\n",
       " 'say',\n",
       " 'us',\n",
       " 'nevertheless',\n",
       " 'some',\n",
       " 're',\n",
       " 'anywhere',\n",
       " 'various',\n",
       " 'myself',\n",
       " 'whenever',\n",
       " 'them',\n",
       " 'himself',\n",
       " 'upon',\n",
       " 'you',\n",
       " 'whereupon',\n",
       " 'therein',\n",
       " 'everyone',\n",
       " 'seem',\n",
       " 'those',\n",
       " 'again',\n",
       " 'alone',\n",
       " 'nor',\n",
       " 'empty',\n",
       " 'move',\n",
       " 'can',\n",
       " 'due',\n",
       " 'make',\n",
       " 'always',\n",
       " 'are',\n",
       " 'hundred',\n",
       " 'seemed',\n",
       " 'being',\n",
       " 'all',\n",
       " 'afterwards',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'anyhow',\n",
       " 'about',\n",
       " 'else',\n",
       " 'although',\n",
       " 'nine',\n",
       " 'sometime',\n",
       " 'thru',\n",
       " 'see',\n",
       " 'he',\n",
       " 'twelve',\n",
       " 'twenty',\n",
       " 'nobody',\n",
       " 'since',\n",
       " 'two',\n",
       " 'five',\n",
       " 'others',\n",
       " 'already',\n",
       " 'besides',\n",
       " 'something',\n",
       " 'hereby',\n",
       " 'because',\n",
       " 'from',\n",
       " 'during',\n",
       " 'under',\n",
       " 'then',\n",
       " 'meanwhile',\n",
       " 'done',\n",
       " 'several',\n",
       " 'per',\n",
       " 'without',\n",
       " 'forty',\n",
       " 'beforehand',\n",
       " 'beyond',\n",
       " 'anything',\n",
       " 'please',\n",
       " 'for',\n",
       " 'her',\n",
       " 'sometimes',\n",
       " 'next',\n",
       " 'doing',\n",
       " 'not',\n",
       " 'serious',\n",
       " 'therefore',\n",
       " 'an',\n",
       " 'the',\n",
       " 'everything',\n",
       " 'out',\n",
       " 'though',\n",
       " 'towards',\n",
       " 'almost',\n",
       " 'is',\n",
       " 'had',\n",
       " 'back',\n",
       " 'must',\n",
       " 'own',\n",
       " 'amongst',\n",
       " 'side',\n",
       " 'much',\n",
       " 'go',\n",
       " 'which',\n",
       " 'using',\n",
       " 'they',\n",
       " 'who',\n",
       " 'whose',\n",
       " 'above',\n",
       " 'up',\n",
       " 'through',\n",
       " 'where',\n",
       " 'wherein',\n",
       " 'otherwise',\n",
       " 'give',\n",
       " 'yours',\n",
       " 'same',\n",
       " 'least',\n",
       " 'themselves',\n",
       " 'never',\n",
       " 'whereafter',\n",
       " 'used',\n",
       " 'name',\n",
       " 'nothing',\n",
       " 'so',\n",
       " 'was',\n",
       " 'wherever',\n",
       " 'whether',\n",
       " 'thence',\n",
       " 'have',\n",
       " 'keep',\n",
       " 'only',\n",
       " 'this',\n",
       " 'front',\n",
       " 'these',\n",
       " 'does',\n",
       " 'another',\n",
       " 'regarding',\n",
       " 'and',\n",
       " 'anyone',\n",
       " 'former',\n",
       " 'off',\n",
       " 'rather',\n",
       " 'our',\n",
       " 'six',\n",
       " 'became',\n",
       " 'bottom',\n",
       " 'herein',\n",
       " 'each',\n",
       " 'itself',\n",
       " 'me',\n",
       " 'that',\n",
       " 'too',\n",
       " 'down',\n",
       " 'when',\n",
       " 'namely',\n",
       " 'his',\n",
       " 'my',\n",
       " 'whom',\n",
       " 'thereafter',\n",
       " 'around',\n",
       " 'become',\n",
       " 'beside',\n",
       " 'in',\n",
       " 'sixty',\n",
       " 'indeed',\n",
       " 'via',\n",
       " 'yourself',\n",
       " 'noone',\n",
       " 'none',\n",
       " 'thus',\n",
       " 'well',\n",
       " 'fifteen',\n",
       " 'neither',\n",
       " 'will',\n",
       " 'any',\n",
       " 'once',\n",
       " 'ever',\n",
       " 'do',\n",
       " 'seems',\n",
       " 'may',\n",
       " 'with',\n",
       " 'call',\n",
       " 'along',\n",
       " 'to',\n",
       " 'becomes',\n",
       " 'she',\n",
       " 'such',\n",
       " 'a',\n",
       " 'onto',\n",
       " 'eight',\n",
       " 'further',\n",
       " 'across',\n",
       " 'first',\n",
       " 'full',\n",
       " 'quite',\n",
       " 'their',\n",
       " 'we',\n",
       " 'whereby',\n",
       " 'within',\n",
       " 'less',\n",
       " 'below',\n",
       " 'against',\n",
       " 'nowhere',\n",
       " 'been',\n",
       " 'also',\n",
       " 'there',\n",
       " 'anyway',\n",
       " 'behind',\n",
       " 'ours',\n",
       " 'here',\n",
       " 'hereafter',\n",
       " 'four',\n",
       " 'third',\n",
       " 'among',\n",
       " 'be',\n",
       " 'at',\n",
       " 'either',\n",
       " 'if',\n",
       " 'ten',\n",
       " 'many',\n",
       " 'might',\n",
       " 'moreover',\n",
       " 'very',\n",
       " 'while',\n",
       " 'ca',\n",
       " 'it',\n",
       " 'elsewhere',\n",
       " 'however',\n",
       " 'part',\n",
       " 'did',\n",
       " 'herself',\n",
       " 'until',\n",
       " 'every',\n",
       " 'fifty',\n",
       " 'whatever',\n",
       " 'between',\n",
       " 'of',\n",
       " 'really',\n",
       " 'how',\n",
       " 'eleven',\n",
       " 'three',\n",
       " 'no',\n",
       " 'yet',\n",
       " 'except',\n",
       " 'mine',\n",
       " 'made',\n",
       " 'throughout',\n",
       " 'over',\n",
       " 'both']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = list(STOP_WORDS)\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "docx = nlp(\"Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story Lemma => Story\n",
      "of Lemma => of\n",
      "a Lemma => a\n",
      "man Lemma => man\n",
      "who Lemma => who\n",
      "has Lemma => have\n",
      "unnatural Lemma => unnatural\n",
      "feelings Lemma => feeling\n",
      "for Lemma => for\n",
      "a Lemma => a\n",
      "pig Lemma => pig\n",
      ". Lemma => .\n",
      "Starts Lemma => Starts\n",
      "out Lemma => out\n",
      "with Lemma => with\n",
      "a Lemma => a\n",
      "opening Lemma => open\n",
      "scene Lemma => scene\n",
      "that Lemma => that\n",
      "is Lemma => be\n",
      "a Lemma => a\n",
      "terrific Lemma => terrific\n",
      "example Lemma => example\n",
      "of Lemma => of\n",
      "absurd Lemma => absurd\n",
      "comedy Lemma => comedy\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizing of tokens\n",
    "for word in docx:\n",
    "    print(word.text,\"Lemma =>\",word.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story\n",
      "man\n",
      "unnatural\n",
      "feelings\n",
      "pig\n",
      "Starts\n",
      "opening\n",
      "scene\n",
      "terrific\n",
      "example\n",
      "absurd\n",
      "comedy\n"
     ]
    }
   ],
   "source": [
    "# Filtering out Stopwords and Punctuations\n",
    "for word in docx:\n",
    "    if word.is_stop == False and not word.is_punct:\n",
    "        if word.is_stop != True and not word.is_punct:\n",
    "            print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Story,\n",
       " man,\n",
       " unnatural,\n",
       " feelings,\n",
       " pig,\n",
       " Starts,\n",
       " opening,\n",
       " scene,\n",
       " terrific,\n",
       " example,\n",
       " absurd,\n",
       " comedy]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stop words and Punctuation In List Comprehension\n",
    "[ word for word in docx if word.is_stop == False and not word.is_punct ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the punctuations of string module\n",
    "import string\n",
    "punctuations = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Spacy Parser\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(sentence):\n",
    "    mytokens = parser(sentence)\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "    mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ]\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine Learning With SKlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML Packages\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.base import TransformerMixin \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom transformer using spaCy \n",
    "class predictors(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        return [clean_text(text) for text in X]\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "# Basic function to clean the text \n",
    "def clean_text(text):     \n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization\n",
    "vectorizer = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1)) \n",
    "knnClassifier = KNeighborsClassifier(n_neighbors=2)\n",
    "mNbClassifier = MultinomialNB()\n",
    "svmClassifier = svm.SVC()\n",
    "votingClassifier = VotingClassifier(estimators=[('LinearSVC', knnClassifier),\n",
    "                                                ('MultinomialNaiveBayes', mNbClassifier), \n",
    "                                                ('SVM', svmClassifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Tfidf\n",
    "tfvectorizer = TfidfVectorizer(tokenizer = spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Data Set\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Features and Labels\n",
    "X = dataTrain['review']\n",
    "ylabels = dataTrain['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the  pipeline to clean, tokenize, vectorize, and classify \n",
    "pipe = Pipeline([(\"cleaner\", predictors()),\n",
    "                 ('vectorizer', vectorizer),\n",
    "                 ('to_dense', DenseTransformer()),\n",
    "                 ('classifier', votingClassifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit our data\n",
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting with a test dataset\n",
    "sample_prediction = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untuk kesekian kali nya jubir jubir jokowi di buat tak berkutik dan menahan rasa malu dan Aktor Protagonis yg melakukan Prediction=> 0.0\n",
      "2 9jokowilanjut Prediction=> 1.0\n",
      "TetapIndonesia JokowiLagi Prediction=> 1.0\n",
      "cek gt Gara gara Iklan Videotron Jokowi Amin Dikejar Syahroni Sidang dugaan pelangga Prediction=> 0.0\n",
      "Ssstt Ssstt HATTRICK Prediction=> 0.0\n",
      "Kpop bagus Via valen bagus Nela karisma bagus Jokowi bagus Prabowo bagus Toko bagus Yang jelek cuman tulisan tangan saya Prediction=> 1.0\n",
      "Yok bisa yok Siaran Pers Peluncuran Laporan Evaluasi 4 Tahun Pemerintahan JK di sektor HAM oleh KontraS Jumat 9 Ok Prediction=> 1.0\n",
      "Ini logika yg merusak pak karena nyalahin padahal maksudnya nenbak gubernur DKI sebelumnya kan Waspada Prediction=> 0.0\n",
      "JokowiLagi Prediction=> 1.0\n",
      "JokowiAmin jokowimarufamin Prediction=> 0.0\n",
      "Saya menyampaikan selamat kepada sahabat saya Bapak Prabowo Subianto yang merayakan ulang tahun yang ke 67 pada hari ini Ter Prediction=> 0.0\n",
      "Di Sumbawa saya saksikan bantuan pemerintah untuk korban gempa sudah gampang diuangkan Jangan lupa yang rusak di NTB ini t Prediction=> 1.0\n",
      "Sejak menjabat Anies satu per satu janji dipenuhi Jokowi satu per satu janji diingkari Pergub OK OCE Diteken Anies Bu Prediction=> 0.0\n",
      "jokowi Siapa pun Cawapres intinya Jokowi harus 2 periode Prediction=> 1.0\n",
      "arsyad Seharusnya presiden menayakan dlu kpd staff nya masalah hierarki dari bpjs ini agar tidak menambah bodoh presiden di m Prediction=> 0.0\n",
      "Semoga sukses lancar Prediction=> 1.0\n",
      "suka ga sadar kalo banyak utang nih jokowi Prediction=> 1.0\n",
      "Kami gt Ex VIJ siap mengawal dgn segala kompetensi dan pengalaman kami saatJamkesmas Terbukti 2 8 2 9 t Prediction=> 1.0\n",
      "Kembali menuju NTB untuk meninjau proses rehabilitasi dan rekonstruksi pasca gempa di Lombok dan sekitarnya Saya hendak memas Prediction=> 1.0\n",
      "open discuss ya kurasa petahana memang punya advantage seperti ini Praktik simplenya Petahana akan se Prediction=> 1.0\n",
      "JokowiLagi bersihmerakyatkerjanyata Jokowi2periode Jokowi inside Penerbitan sertifikat tanah secara masif Prediction=> 1.0\n",
      "Anti Korupsi nya OMDO Temans Kami ikut sedih Ternyata tak lebih baik dari era Prediction=> 0.0\n",
      "Prabowo harusnya berhati2 terima informasi agar tak termakan hoax bagaimana bila nanti memimpin Indonesia klo tak hat Prediction=> 1.0\n",
      "Blunder sendiri jadi boomerang Mungkin doi ga kepikir sampe sejauh itu sekolam Prediction=> 0.0\n",
      "Bupati pendukung Jokowi diciduk KPK tim Prabowo bersyukur tak libatkan kepala daerah Prediction=> 0.0\n",
      "malang Pasang Iklan di Media Cetak Jokowi Ma ruf Diduga Langgar Kampanye Prediction=> 0.0\n",
      "Natalius Pigai Tidak ada Pembukaan ruas jalan baru yg dibangun oleh presiden jokowi Tukimin dipercaya dongo lu Prediction=> 0.0\n",
      "RS dipecat dari Timses Prabowo Sandi negara tidak ada kerugian digoreng terus menerus ST Prediction=> 1.0\n",
      "Stone Kardol Aamiin ya rabbal alamin Prediction=> 0.0\n",
      "Jokowi periksa got gt pencitraan Jokowi periksa irigasi gt pencitraan Jokowi bincang dgn petani gt pencitraan Sandi Prediction=> 0.0\n",
      "JokowiLagi Prediction=> 1.0\n",
      "Pak Jokowi BPJS Kesehatan adalah program strategis nasional yang oleh bapak disebut dengan Jaminan Kesehatan Nasional Kar Prediction=> 0.0\n",
      "Pasang Iklan di Media Cetak Jokowi Ma ruf Diduga Langgar Kampanye Prediction=> 0.0\n",
      "jkt Tau ah geyyyyaapppp Prediction=> 1.0\n",
      "JokowiLagi Sumber htt Prediction=> 1.0\n",
      "Kata Jokowi di TV mestinya adu program adu ide etc Apanya yg diadu Sebab 66 program pilpres 2 4 mangkrak Mau yg mangk Prediction=> 0.0\n",
      "Presiden kan bagikan 5 beasiswa kpd mahasiswa yg terkena imbas bencana Lombok dr 5 beasiswa tersebut Prediction=> 0.0\n",
      "Jokowi itu manager Do the things right melakukan sesuatu yang benar Prabowo itu leader Do the right things melakukan Prediction=> 0.0\n",
      "Memilih Jokowi sebagai pemimpin dengan alasan tak ingin Prabowo berkuasa adalah hal paling menyedihkan yg pernah gw dengar I Prediction=> 0.0\n",
      "Anak Buah Prabowo Prihatin Jurkam Jokowi Terima Suap Prediction=> 0.0\n",
      "HBS OPERASI PLASTIK KPN MRK OPS KELAMIN YA Pak jokowi merokok Prediction=> 0.0\n",
      "2 9TetapJokowi Prediction=> 1.0\n",
      "cek yuk gt Gara gara Iklan Videotron Jokowi Amin Dikejar Syahroni Sidang dugaan pelangga Prediction=> 0.0\n"
     ]
    }
   ],
   "source": [
    "# Prediction Results\n",
    "# 1 = Positive review\n",
    "# 0 = Negative review\n",
    "for (sample,pred) in zip(X_test,sample_prediction):\n",
    "    print(sample,\"Prediction=>\",pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7441860465116279\n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "print(\"Accuracy: \",pipe.score(X_test,y_test))\n",
    "print(\"Accuracy: \",pipe.score(X_test,sample_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9883720930232558\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "print(\"Accuracy: \",pipe.score(X_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0.0'], dtype='<U5')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another random review\n",
    "# pipe.predict([\"jokowi Siapa pun Cawapres intinya Jokowi harus 2 periode\"])\n",
    "pipe.predict([\"Memilih Jokowi sebagai pemimpin dengan alasan tak ingin Prabowo berkuasa adalah hal paling menyedihkan yg pernah gw dengar\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Predicting with a test dataset\n",
    "sample_prediction = pipe.predict(X_test)\n",
    "report = classification_report(y_test, sample_prediction)\n",
    "print(report)\n",
    "\n",
    "confusion = confusion_matrix(y_test, sample_prediction)\n",
    "print(confusion)\n",
    "#[row, column]\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]\n",
    "\n",
    "print(FP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
